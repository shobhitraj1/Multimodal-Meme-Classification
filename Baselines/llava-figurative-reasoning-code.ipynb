{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Load the model in half-precision\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\n",
    "    r'^ASSISTANT:\\s*.*1\\.\\s*Cause-Effect:\\s*(.*?)\\s*2\\.\\s*Figurative Understanding:\\s*(.*?)\\s*3\\.\\s*Mental State:\\s*(.*)$',\n",
    "    re.DOTALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set required attributes for image handling\n",
    "processor.patch_size = 14\n",
    "processor.vision_feature_select_strategy = \"default\"\n",
    "\n",
    "# Define the instruction for each image\n",
    "instruction = \"\"\"Analyze the following depression meme image to extract common sense reasoning in the form of triples. These relationships should\n",
    "capture the following elements:\n",
    "• 1. Cause-Effect: Identify concrete causes or results of the situation depicted in the meme.\n",
    "• 2. Figurative Understanding: Capture underlying metaphors, analogies, or symbolic meanings that convey the meme's deeper message, including any ironic or humorous undertones.\n",
    "• 3. Mental State: Capture specific mental or emotional states depicted in the meme.\"\"\"\n",
    "\n",
    "# Directory containing training images\n",
    "train_dir = \"/kaggle/input/meme-dataset/meme-ocr\"\n",
    "output_data = []\n",
    "cnt = 0\n",
    "\n",
    "# Process each image in the \"train\" directory\n",
    "for filename in tqdm(os.listdir(train_dir), desc=\"Processing images\"):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "        # Open the image\n",
    "        image_path = os.path.join(train_dir, filename)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Construct conversation input for the model\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"url\": \"local://dummy\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": instruction}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Process the conversation into model inputs\n",
    "        inputs = processor.apply_chat_template(\n",
    "            conversation,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Compute image features manually if needed\n",
    "        if \"pixel_values\" not in inputs or inputs[\"pixel_values\"] is None:\n",
    "            pixel_values = processor.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "            inputs[\"pixel_values\"] = pixel_values.to(model.device, torch.float16)\n",
    "\n",
    "        # Generate a response\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=300)\n",
    "        response_text = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "\n",
    "        # while True:\n",
    "        #     # Generate a response\n",
    "        #     generate_ids = model.generate(**inputs, max_new_tokens=300, temperature=0.5, do_sample = True)\n",
    "        #     response_text = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "    \n",
    "        #     # Extract model's response after user's prompt\n",
    "        #     if response_text:\n",
    "        #         model_response = response_text[0].split(instruction, 1)[-1].strip()\n",
    "        #         match = re.match(pattern, model_response)\n",
    "        #         if match:\n",
    "        #             break\n",
    "\n",
    "        # Extract model's response after user's prompt\n",
    "        if response_text:\n",
    "            model_response = response_text[0].split(instruction, 1)[-1].strip()\n",
    "\n",
    "            # Store the response in the required JSON format\n",
    "            sample_id = filename.split(\".\")[0]  # Extract sample ID from filename\n",
    "            output_data.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"figurative_reasoning\": model_response\n",
    "            })\n",
    "\n",
    "        # Increment the counter\n",
    "        cnt += 1\n",
    "\n",
    "        # Dump the content into a JSON file after every 100 images\n",
    "        if cnt % 100 == 0:\n",
    "            output_file = f\"train_figurative_{cnt}.json\"\n",
    "            with open(output_file, \"w\") as json_file:\n",
    "                json.dump(output_data, json_file, indent=4)\n",
    "            print(f\"Responses saved to {output_file}\")\n",
    "\n",
    "    # Save any remaining responses in a JSON file\n",
    "    if output_data:\n",
    "        output_file = f\"train_figurative_{cnt}.json\"\n",
    "        with open(output_file, \"w\") as json_file:\n",
    "            json.dump(output_data, json_file, indent=4)\n",
    "        print(f\"Responses saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
