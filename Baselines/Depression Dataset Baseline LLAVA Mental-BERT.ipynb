{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Checkpoint Drive Link: ([Mental-BERT LLAVA](https://drive.google.com/file/d/1ISBSZzCrlp5lDtW7Idnxjk4bfaY_DRVd/view?usp=sharing))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:38:48.776659Z",
     "iopub.status.busy": "2025-03-25T16:38:48.776445Z",
     "iopub.status.idle": "2025-03-25T16:38:52.975133Z",
     "shell.execute_reply": "2025-03-25T16:38:52.974462Z",
     "shell.execute_reply.started": "2025-03-25T16:38:48.776638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:38:52.976418Z",
     "iopub.status.busy": "2025-03-25T16:38:52.975942Z",
     "iopub.status.idle": "2025-03-25T16:38:53.032207Z",
     "shell.execute_reply": "2025-03-25T16:38:53.031126Z",
     "shell.execute_reply.started": "2025-03-25T16:38:52.976385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "LABELS = [\"Lack of Interest\", \"Feeling Down\", \"Eating Disorder\",\n",
    "          \"Sleeping Disorder\", \"Low Self-Esteem\", \"Concentration Problem\", \"Self-Harm\"]\n",
    "LABEL_MAP = {label: i for i, label in enumerate(LABELS)}\n",
    "NUM_CLASSES = len(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:38:56.651032Z",
     "iopub.status.busy": "2025-03-25T16:38:56.650726Z",
     "iopub.status.idle": "2025-03-25T16:38:56.657037Z",
     "shell.execute_reply": "2025-03-25T16:38:56.656097Z",
     "shell.execute_reply.started": "2025-03-25T16:38:56.651009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DepressionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_map, max_len=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        text = sample[\"ocr_text\"] + \" \" + sample[\"figurative_reasoning\"]\n",
    "        labels = sample[\"meme_depressive_categories\"]\n",
    "\n",
    "        # Convert label list to one-hot encoding\n",
    "        label_tensor = torch.zeros(len(self.label_map))\n",
    "        for label in labels:\n",
    "            label_tensor[self.label_map[label]] = 1\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": label_tensor,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:38:57.240182Z",
     "iopub.status.busy": "2025-03-25T16:38:57.239898Z",
     "iopub.status.idle": "2025-03-25T16:38:57.371997Z",
     "shell.execute_reply": "2025-03-25T16:38:57.371275Z",
     "shell.execute_reply.started": "2025-03-25T16:38:57.240161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = json.load(open(\"/kaggle/input/llava-data/depression_train_llava_dataset.json\", \"r\"))\n",
    "val_data = json.load(open(\"/kaggle/input/llava-data/depression_val_llava_dataset.json\", \"r\"))\n",
    "test_data = json.load(open(\"/kaggle/input/llava-data/depression_test_llava_dataset.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:38:58.983890Z",
     "iopub.status.busy": "2025-03-25T16:38:58.983584Z",
     "iopub.status.idle": "2025-03-25T16:38:58.990111Z",
     "shell.execute_reply": "2025-03-25T16:38:58.989239Z",
     "shell.execute_reply.started": "2025-03-25T16:38:58.983868Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Data: 8722\n",
      "Size of Validation Data: 359\n",
      "Size of Test Data: 520\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training Data:\", len(train_data))\n",
    "print(\"Size of Validation Data:\", len(val_data))\n",
    "print(\"Size of Test Data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:38:59.199714Z",
     "iopub.status.busy": "2025-03-25T16:38:59.199472Z",
     "iopub.status.idle": "2025-03-25T16:38:59.211408Z",
     "shell.execute_reply": "2025-03-25T16:38:59.210671Z",
     "shell.execute_reply.started": "2025-03-25T16:38:59.199694Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Train dataset:\n",
      "  Lack of Interest: 471\n",
      "  Feeling Down: 2085\n",
      "  Eating Disorder: 1939\n",
      "  Sleeping Disorder: 1562\n",
      "  Low Self-Esteem: 855\n",
      "  Concentration Problem: 595\n",
      "  Self-Harm: 1516\n",
      "-----------------------------------\n",
      "Class distribution in Validation dataset:\n",
      "  Lack of Interest: 45\n",
      "  Feeling Down: 195\n",
      "  Eating Disorder: 49\n",
      "  Sleeping Disorder: 45\n",
      "  Low Self-Esteem: 85\n",
      "  Concentration Problem: 42\n",
      "  Self-Harm: 61\n",
      "-----------------------------------\n",
      "Class distribution in Test dataset:\n",
      "  Lack of Interest: 71\n",
      "  Feeling Down: 218\n",
      "  Eating Disorder: 92\n",
      "  Sleeping Disorder: 79\n",
      "  Low Self-Esteem: 114\n",
      "  Concentration Problem: 66\n",
      "  Self-Harm: 81\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_class_distribution(dataset, dataset_name):\n",
    "    label_counts = Counter([LABEL_MAP[i] for item in dataset for i in item[\"meme_depressive_categories\"]])\n",
    "    print(f\"Class distribution in {dataset_name} dataset:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"  {LABELS[label]}: {count}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "print_class_distribution(train_data, \"Train\")\n",
    "print_class_distribution(val_data, \"Validation\")\n",
    "print_class_distribution(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:39:00.884071Z",
     "iopub.status.busy": "2025-03-25T16:39:00.883777Z",
     "iopub.status.idle": "2025-03-25T16:39:00.894661Z",
     "shell.execute_reply": "2025-03-25T16:39:00.893637Z",
     "shell.execute_reply.started": "2025-03-25T16:39:00.884049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_data, val_data, epochs, model_save_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = DepressionDataset(train_data, tokenizer, LABEL_MAP)\n",
    "    val_dataset = DepressionDataset(val_data, tokenizer, LABEL_MAP)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"].to(DEVICE),\n",
    "                batch[\"attention_mask\"].to(DEVICE),\n",
    "                batch[\"labels\"].to(DEVICE),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, labels=labels)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.sigmoid(outputs.logits).detach().cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute train F1 scores\n",
    "        train_macro_f1, train_weighted_f1 = compute_f1_scores(train_labels, train_preds)\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Train Macro-F1: {train_macro_f1:.4f}, Weighted-F1: {train_weighted_f1:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        val_loss, val_macro_f1, val_weighted_f1 = evaluate_model(model, val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro-F1: {val_macro_f1:.4f}, Weighted-F1: {val_weighted_f1:.4f}\")\n",
    "\n",
    "        f1_hm = 2 * val_macro_f1 * val_weighted_f1 / (val_macro_f1 + val_weighted_f1)\n",
    "\n",
    "        # Save best model\n",
    "        if f1_hm > best_val_f1:\n",
    "            best_val_f1 = f1_hm\n",
    "            torch.save(model.state_dict(), f\"{model_save_name}_depression_model.pth\")\n",
    "            print(\"Best model saved!\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    preds, labels = [], []\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids, attention_mask, labels_batch = (\n",
    "                batch[\"input_ids\"].to(DEVICE),\n",
    "                batch[\"attention_mask\"].to(DEVICE),\n",
    "                batch[\"labels\"].to(DEVICE),\n",
    "            )\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, labels=labels_batch)\n",
    "            loss += criterion(outputs.logits, labels_batch).item()\n",
    "\n",
    "            preds.extend(torch.sigmoid(outputs.logits).cpu().numpy())\n",
    "            labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "    macro_f1, weighted_f1 = compute_f1_scores(labels, preds)\n",
    "    return loss / len(loader), macro_f1, weighted_f1\n",
    "\n",
    "def compute_f1_scores(true_labels, pred_probs, threshold=0.5):\n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_probs = np.array(pred_probs)\n",
    "    pred_labels = (pred_probs > threshold).astype(int)\n",
    "\n",
    "    macro_f1 = f1_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    weighted_f1 = f1_score(true_labels, pred_labels, average=\"weighted\", zero_division=0)\n",
    "    return macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + Mental-BERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:39:05.542481Z",
     "iopub.status.busy": "2025-03-25T16:39:05.542184Z",
     "iopub.status.idle": "2025-03-25T17:55:26.643799Z",
     "shell.execute_reply": "2025-03-25T17:55:26.642556Z",
     "shell.execute_reply.started": "2025-03-25T16:39:05.542457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:47<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2885\n",
      "Train Macro-F1: 0.4916, Weighted-F1: 0.5307\n",
      "Validation Loss: 0.3607\n",
      "Validation Macro-F1: 0.5548, Weighted-F1: 0.5101\n",
      "Best model saved!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1788\n",
      "Train Macro-F1: 0.7479, Weighted-F1: 0.7664\n",
      "Validation Loss: 0.3771\n",
      "Validation Macro-F1: 0.5709, Weighted-F1: 0.5616\n",
      "Best model saved!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1262\n",
      "Train Macro-F1: 0.8309, Weighted-F1: 0.8456\n",
      "Validation Loss: 0.4151\n",
      "Validation Macro-F1: 0.5814, Weighted-F1: 0.5484\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0553\n",
      "Train Macro-F1: 0.9388, Weighted-F1: 0.9431\n",
      "Validation Loss: 0.4963\n",
      "Validation Macro-F1: 0.5898, Weighted-F1: 0.5852\n",
      "Best model saved!\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0504\n",
      "Train Macro-F1: 0.9485, Weighted-F1: 0.9493\n",
      "Validation Loss: 0.5656\n",
      "Validation Macro-F1: 0.5523, Weighted-F1: 0.5539\n",
      "Best model saved!\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0476\n",
      "Train Macro-F1: 0.9523, Weighted-F1: 0.9564\n",
      "Validation Loss: 0.6182\n",
      "Validation Macro-F1: 0.5473, Weighted-F1: 0.5459\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0427\n",
      "Train Macro-F1: 0.9645, Weighted-F1: 0.9640\n",
      "Validation Loss: 0.8724\n",
      "Validation Macro-F1: 0.5111, Weighted-F1: 0.5134\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0419\n",
      "Train Macro-F1: 0.9624, Weighted-F1: 0.9623\n",
      "Validation Loss: 0.9757\n",
      "Validation Macro-F1: 0.4995, Weighted-F1: 0.4908\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0379\n",
      "Train Macro-F1: 0.9715, Weighted-F1: 0.9713\n",
      "Validation Loss: 0.9824\n",
      "Validation Macro-F1: 0.5282, Weighted-F1: 0.5201\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [14:46<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0401\n",
      "Train Macro-F1: 0.9655, Weighted-F1: 0.9665\n",
      "Validation Loss: 0.9584\n",
      "Validation Macro-F1: 0.5145, Weighted-F1: 0.5166\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mental/mental-bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES, problem_type=\"multi_label_classification\").to(DEVICE)\n",
    "model_mental_bert, tokenizer_mental_bert = train_model(model, model_name, train_data, val_data, EPOCHS, \"mental_bert_llava\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + Mental-BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T17:56:18.241109Z",
     "iopub.status.busy": "2025-03-25T17:56:18.240815Z",
     "iopub.status.idle": "2025-03-25T17:56:37.198090Z",
     "shell.execute_reply": "2025-03-25T17:56:37.197064Z",
     "shell.execute_reply.started": "2025-03-25T17:56:18.241087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Evaluation:\n",
      "Test Loss: 0.4334\n",
      "Test Macro-F1: 0.6298\n",
      "Test Weighted-F1: 0.6263\n"
     ]
    }
   ],
   "source": [
    "test_dataset_mental_bert = DepressionDataset(test_data, tokenizer_mental_bert, LABEL_MAP)\n",
    "test_loader_mental_bert = DataLoader(test_dataset_mental_bert, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_mental_bert.load_state_dict(torch.load(\"mental_bert_llava_depression_model.pth\", weights_only=True))\n",
    "test_loss, test_macro_f1, test_weighted_f1 = evaluate_model(model_mental_bert, test_loader_mental_bert)\n",
    "\n",
    "print(f\"\\nFinal Test Evaluation:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
    "print(f\"Test Weighted-F1: {test_weighted_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6965270,
     "sourceId": 11162322,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6966415,
     "sourceId": 11163859,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
