{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Checkpoint Drive Link: ([Mental-BERT](https://drive.google.com/file/d/1zIHYksEZFY-dE_s6yPTdhSJWXuPSEdis/view?usp=drive_link))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:28:30.845900Z",
     "iopub.status.busy": "2025-03-25T16:28:30.845514Z",
     "iopub.status.idle": "2025-03-25T16:28:39.587627Z",
     "shell.execute_reply": "2025-03-25T16:28:39.586969Z",
     "shell.execute_reply.started": "2025-03-25T16:28:30.845866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use 70:10:20 train:val:test split as specified in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:28:39.589087Z",
     "iopub.status.busy": "2025-03-25T16:28:39.588679Z",
     "iopub.status.idle": "2025-03-25T16:28:39.661048Z",
     "shell.execute_reply": "2025-03-25T16:28:39.659963Z",
     "shell.execute_reply.started": "2025-03-25T16:28:39.589052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 2e-5\n",
    "TRAIN_RATIO = 0.7  # 70% train\n",
    "VAL_RATIO = 0.1  # 10% validation\n",
    "TEST_RATIO = 0.2 # 20% test\n",
    "\n",
    "LABELS = [\"Nervousness\", \"Lack of Worry Control\", \"Excessive Worry\", \n",
    "          \"Difficulty Relaxing\", \"Restlessness\", \"Impending Doom\"]\n",
    "LABEL_MAP = {label: i for i, label in enumerate(LABELS)}\n",
    "NUM_CLASSES = len(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:30:45.987921Z",
     "iopub.status.busy": "2025-03-25T16:30:45.987620Z",
     "iopub.status.idle": "2025-03-25T16:30:45.993380Z",
     "shell.execute_reply": "2025-03-25T16:30:45.992689Z",
     "shell.execute_reply.started": "2025-03-25T16:30:45.987894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AnxietyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_map, max_len=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        text = sample[\"ocr_text\"] + \" \" + sample[\"figurative_reasoning\"]\n",
    "        label = self.label_map[sample[\"meme_anxiety_category\"]]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:30:53.312658Z",
     "iopub.status.busy": "2025-03-25T16:30:53.312389Z",
     "iopub.status.idle": "2025-03-25T16:30:53.351063Z",
     "shell.execute_reply": "2025-03-25T16:30:53.350463Z",
     "shell.execute_reply.started": "2025-03-25T16:30:53.312638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "full_train_data = json.load(open(\"/kaggle/input/anxiety-llava/anxiety_train_llava_dataset.json\", \"r\"))\n",
    "test_data = json.load(open(\"/kaggle/input/anxiety-llava/anxiety_test_llava_dataset.json\", \"r\"))\n",
    "\n",
    "labels = [LABEL_MAP[item[\"meme_anxiety_category\"]] for item in full_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:30:53.767891Z",
     "iopub.status.busy": "2025-03-25T16:30:53.767567Z",
     "iopub.status.idle": "2025-03-25T16:30:53.778481Z",
     "shell.execute_reply": "2025-03-25T16:30:53.777624Z",
     "shell.execute_reply.started": "2025-03-25T16:30:53.767864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train-val split\n",
    "\n",
    "train_size = math.ceil(len(full_train_data) * TRAIN_RATIO / (TRAIN_RATIO + VAL_RATIO))\n",
    "train_data, val_data = train_test_split(\n",
    "    full_train_data, train_size=train_size, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:30:54.204718Z",
     "iopub.status.busy": "2025-03-25T16:30:54.204473Z",
     "iopub.status.idle": "2025-03-25T16:30:54.215597Z",
     "shell.execute_reply": "2025-03-25T16:30:54.214796Z",
     "shell.execute_reply.started": "2025-03-25T16:30:54.204698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Train dataset:\n",
      "  Nervousness: 373\n",
      "  Lack of Worry Control: 331\n",
      "  Excessive Worry: 322\n",
      "  Difficulty Relaxing: 356\n",
      "  Restlessness: 405\n",
      "  Impending Doom: 366\n",
      "-----------------------------------\n",
      "Class distribution in Validation dataset:\n",
      "  Nervousness: 53\n",
      "  Lack of Worry Control: 47\n",
      "  Excessive Worry: 46\n",
      "  Difficulty Relaxing: 51\n",
      "  Restlessness: 58\n",
      "  Impending Doom: 52\n",
      "-----------------------------------\n",
      "Class distribution in Test dataset:\n",
      "  Nervousness: 106\n",
      "  Lack of Worry Control: 94\n",
      "  Excessive Worry: 92\n",
      "  Difficulty Relaxing: 102\n",
      "  Restlessness: 116\n",
      "  Impending Doom: 105\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_class_distribution(dataset, dataset_name):\n",
    "    label_counts = Counter([LABEL_MAP[item[\"meme_anxiety_category\"]] for item in dataset])\n",
    "    print(f\"Class distribution in {dataset_name} dataset:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"  {LABELS[label]}: {count}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "print_class_distribution(train_data, \"Train\")\n",
    "print_class_distribution(val_data, \"Validation\")\n",
    "print_class_distribution(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:30:56.524328Z",
     "iopub.status.busy": "2025-03-25T16:30:56.523979Z",
     "iopub.status.idle": "2025-03-25T16:30:56.534610Z",
     "shell.execute_reply": "2025-03-25T16:30:56.533670Z",
     "shell.execute_reply.started": "2025-03-25T16:30:56.524299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_data, val_data, epochs, model_save_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataset = AnxietyDataset(train_data, tokenizer, LABEL_MAP)\n",
    "    val_dataset = AnxietyDataset(val_data, tokenizer, LABEL_MAP)\n",
    "    \n",
    "    print(\"Train Set Size:\", len(train_dataset))\n",
    "    print(\"Validation Set Size:\", len(val_dataset))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_preds, train_labels = [], []\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"].to(DEVICE),\n",
    "                batch[\"attention_mask\"].to(DEVICE),\n",
    "                batch[\"label\"].to(DEVICE),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute train F1 scores\n",
    "        train_macro_f1 = f1_score(train_labels, train_preds, average=\"macro\")\n",
    "        train_weighted_f1 = f1_score(train_labels, train_preds, average=\"weighted\")\n",
    "\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Train Macro-F1: {train_macro_f1:.4f}, Weighted-F1: {train_weighted_f1:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        val_loss, val_macro_f1, val_weighted_f1 = evaluate_model(model, val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro-F1: {val_macro_f1:.4f}, Weighted-F1: {val_weighted_f1:.4f}\")\n",
    "\n",
    "        f1_hm = 2 * val_macro_f1 * val_weighted_f1 / (val_macro_f1 + val_weighted_f1)\n",
    "\n",
    "        # Save best model\n",
    "        if f1_hm > best_f1:\n",
    "            best_f1 = f1_hm\n",
    "            torch.save(model.state_dict(), f\"{model_save_name}_anxiety_model.pth\")\n",
    "            print(\"Best model saved!\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids, attention_mask, labels_batch = (\n",
    "                batch[\"input_ids\"].to(DEVICE),\n",
    "                batch[\"attention_mask\"].to(DEVICE),\n",
    "                batch[\"label\"].to(DEVICE),\n",
    "            )\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, labels=labels_batch)\n",
    "            loss += outputs.loss.item()\n",
    "\n",
    "            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    return loss / len(loader), macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + LLAVA Figurative Reasoning + Mental-BERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:31:36.568428Z",
     "iopub.status.busy": "2025-03-25T16:31:36.568133Z",
     "iopub.status.idle": "2025-03-25T17:10:35.307747Z",
     "shell.execute_reply": "2025-03-25T17:10:35.306692Z",
     "shell.execute_reply.started": "2025-03-25T16:31:36.568404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3617a269c8b4be3ac806e607192b1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/639 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f145aefe05e84322b77360021c791829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7936901053f94aee9226d343e6c0c9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25034d5478f84f0f8b12d84a8b505b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58f241d3c8b4a5ab119550c1772ff2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab693887a23f4b2f863c6c1e8a61d73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 2153\n",
      "Validation Set Size: 307\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:34<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6380\n",
      "Train Macro-F1: 0.3208, Weighted-F1: 0.3237\n",
      "Validation Loss: 1.2551\n",
      "Validation Macro-F1: 0.4654, Weighted-F1: 0.4625\n",
      "Best model saved!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1379\n",
      "Train Macro-F1: 0.5809, Weighted-F1: 0.5816\n",
      "Validation Loss: 0.9810\n",
      "Validation Macro-F1: 0.6246, Weighted-F1: 0.6256\n",
      "Best model saved!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7922\n",
      "Train Macro-F1: 0.7284, Weighted-F1: 0.7289\n",
      "Validation Loss: 0.9251\n",
      "Validation Macro-F1: 0.6360, Weighted-F1: 0.6353\n",
      "Best model saved!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4711\n",
      "Train Macro-F1: 0.8611, Weighted-F1: 0.8614\n",
      "Validation Loss: 0.9852\n",
      "Validation Macro-F1: 0.6638, Weighted-F1: 0.6633\n",
      "Best model saved!\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2722\n",
      "Train Macro-F1: 0.9264, Weighted-F1: 0.9270\n",
      "Validation Loss: 1.1405\n",
      "Validation Macro-F1: 0.6612, Weighted-F1: 0.6614\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1558\n",
      "Train Macro-F1: 0.9578, Weighted-F1: 0.9582\n",
      "Validation Loss: 1.2149\n",
      "Validation Macro-F1: 0.6406, Weighted-F1: 0.6406\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0997\n",
      "Train Macro-F1: 0.9741, Weighted-F1: 0.9744\n",
      "Validation Loss: 1.3386\n",
      "Validation Macro-F1: 0.6314, Weighted-F1: 0.6284\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0565\n",
      "Train Macro-F1: 0.9854, Weighted-F1: 0.9856\n",
      "Validation Loss: 1.4824\n",
      "Validation Macro-F1: 0.6388, Weighted-F1: 0.6369\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0515\n",
      "Train Macro-F1: 0.9915, Weighted-F1: 0.9916\n",
      "Validation Loss: 1.3958\n",
      "Validation Macro-F1: 0.6535, Weighted-F1: 0.6518\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [03:41<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0466\n",
      "Train Macro-F1: 0.9887, Weighted-F1: 0.9889\n",
      "Validation Loss: 1.4665\n",
      "Validation Macro-F1: 0.6516, Weighted-F1: 0.6497\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mental/mental-bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES).to(DEVICE)\n",
    "model_mental_bert, tokenizer_mental_bert = train_model(model, model_name, train_data, val_data, EPOCHS, \"mental_bert_llava\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + Mental-BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T17:11:33.143707Z",
     "iopub.status.busy": "2025-03-25T17:11:33.143360Z",
     "iopub.status.idle": "2025-03-25T17:11:55.363029Z",
     "shell.execute_reply": "2025-03-25T17:11:55.362111Z",
     "shell.execute_reply.started": "2025-03-25T17:11:33.143667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Size: 615\n",
      "\n",
      "Final Test Evaluation:\n",
      "Test Loss: 1.1704\n",
      "Test Macro-F1: 0.6183\n",
      "Test Weighted-F1: 0.6173\n"
     ]
    }
   ],
   "source": [
    "test_dataset_mental_bert = AnxietyDataset(test_data, tokenizer_mental_bert, LABEL_MAP)\n",
    "print(\"Test Set Size:\", len(test_dataset_mental_bert))\n",
    "test_loader_mental_bert = DataLoader(test_dataset_mental_bert, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_mental_bert.load_state_dict(torch.load(\"mental_bert_llava_anxiety_model.pth\", weights_only=True))\n",
    "test_loss, test_macro_f1, test_weighted_f1 = evaluate_model(model_mental_bert, test_loader_mental_bert)\n",
    "\n",
    "print(f\"\\nFinal Test Evaluation:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
    "print(f\"Test Weighted-F1: {test_weighted_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6954992,
     "sourceId": 11148254,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6964274,
     "sourceId": 11161008,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6966368,
     "sourceId": 11163797,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
