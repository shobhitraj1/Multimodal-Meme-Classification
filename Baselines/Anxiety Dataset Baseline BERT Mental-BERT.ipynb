{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Checkpoint Drive Link: ([BERT](https://drive.google.com/file/d/1NGBUWxXf8YeYLP4f5QpUPyaBLhbVfFhJ/view?usp=drive_link) and [Mental-BERT](https://drive.google.com/file/d/1tTRBfuf99F8jK11G0Q08k_MxIFvXvPfD/view?usp=drive_link))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:29:41.052672Z",
     "iopub.status.busy": "2025-03-25T13:29:41.052417Z",
     "iopub.status.idle": "2025-03-25T13:29:44.637771Z",
     "shell.execute_reply": "2025-03-25T13:29:44.637137Z",
     "shell.execute_reply.started": "2025-03-25T13:29:41.052646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use 70:10:20 train:val:test split as specified in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:29:44.639150Z",
     "iopub.status.busy": "2025-03-25T13:29:44.638774Z",
     "iopub.status.idle": "2025-03-25T13:29:44.687461Z",
     "shell.execute_reply": "2025-03-25T13:29:44.686588Z",
     "shell.execute_reply.started": "2025-03-25T13:29:44.639127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LR = 2e-5\n",
    "TRAIN_RATIO = 0.7  # 70% train\n",
    "VAL_RATIO = 0.1  # 10% validation\n",
    "TEST_RATIO = 0.2 # 20% test\n",
    "\n",
    "LABELS = [\"Nervousness\", \"Lack of Worry Control\", \"Excessive Worry\", \n",
    "          \"Difficulty Relaxing\", \"Restlessness\", \"Impending Doom\"]\n",
    "LABEL_MAP = {label: i for i, label in enumerate(LABELS)}\n",
    "NUM_CLASSES = len(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:29:44.689509Z",
     "iopub.status.busy": "2025-03-25T13:29:44.689294Z",
     "iopub.status.idle": "2025-03-25T13:29:44.701989Z",
     "shell.execute_reply": "2025-03-25T13:29:44.701176Z",
     "shell.execute_reply.started": "2025-03-25T13:29:44.689491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AnxietyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_map, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        text = sample[\"ocr_text\"]\n",
    "        label = self.label_map[sample[\"meme_anxiety_category\"]]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:29:44.703584Z",
     "iopub.status.busy": "2025-03-25T13:29:44.703277Z",
     "iopub.status.idle": "2025-03-25T13:29:44.736078Z",
     "shell.execute_reply": "2025-03-25T13:29:44.735533Z",
     "shell.execute_reply.started": "2025-03-25T13:29:44.703555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "full_train_data = json.load(open(\"/kaggle/input/anxiety-dataset/anxiety_train.json\", \"r\"))\n",
    "test_data = json.load(open(\"/kaggle/input/anxiety-dataset/anxiety_test.json\", \"r\"))\n",
    "\n",
    "labels = [LABEL_MAP[item[\"meme_anxiety_category\"]] for item in full_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:29:44.737135Z",
     "iopub.status.busy": "2025-03-25T13:29:44.736801Z",
     "iopub.status.idle": "2025-03-25T13:29:44.744494Z",
     "shell.execute_reply": "2025-03-25T13:29:44.743736Z",
     "shell.execute_reply.started": "2025-03-25T13:29:44.737078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train-val split\n",
    "\n",
    "train_size = math.ceil(len(full_train_data) * TRAIN_RATIO / (TRAIN_RATIO + VAL_RATIO))\n",
    "train_data, val_data = train_test_split(\n",
    "    full_train_data, train_size=train_size, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:29:44.745562Z",
     "iopub.status.busy": "2025-03-25T13:29:44.745371Z",
     "iopub.status.idle": "2025-03-25T13:29:44.758720Z",
     "shell.execute_reply": "2025-03-25T13:29:44.757986Z",
     "shell.execute_reply.started": "2025-03-25T13:29:44.745544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Train dataset:\n",
      "  Nervousness: 373\n",
      "  Lack of Worry Control: 331\n",
      "  Excessive Worry: 322\n",
      "  Difficulty Relaxing: 356\n",
      "  Restlessness: 405\n",
      "  Impending Doom: 366\n",
      "-----------------------------------\n",
      "Class distribution in Validation dataset:\n",
      "  Nervousness: 53\n",
      "  Lack of Worry Control: 47\n",
      "  Excessive Worry: 46\n",
      "  Difficulty Relaxing: 51\n",
      "  Restlessness: 58\n",
      "  Impending Doom: 52\n",
      "-----------------------------------\n",
      "Class distribution in Test dataset:\n",
      "  Nervousness: 106\n",
      "  Lack of Worry Control: 94\n",
      "  Excessive Worry: 92\n",
      "  Difficulty Relaxing: 102\n",
      "  Restlessness: 116\n",
      "  Impending Doom: 105\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_class_distribution(dataset, dataset_name):\n",
    "    label_counts = Counter([LABEL_MAP[item[\"meme_anxiety_category\"]] for item in dataset])\n",
    "    print(f\"Class distribution in {dataset_name} dataset:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"  {LABELS[label]}: {count}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "print_class_distribution(train_data, \"Train\")\n",
    "print_class_distribution(val_data, \"Validation\")\n",
    "print_class_distribution(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:50:52.527763Z",
     "iopub.status.busy": "2025-03-25T13:50:52.527477Z",
     "iopub.status.idle": "2025-03-25T13:50:52.538396Z",
     "shell.execute_reply": "2025-03-25T13:50:52.537437Z",
     "shell.execute_reply.started": "2025-03-25T13:50:52.527741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_data, val_data, epochs, model_save_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataset = AnxietyDataset(train_data, tokenizer, LABEL_MAP)\n",
    "    val_dataset = AnxietyDataset(val_data, tokenizer, LABEL_MAP)\n",
    "    \n",
    "    print(\"Train Set Size:\", len(train_dataset))\n",
    "    print(\"Validation Set Size:\", len(val_dataset))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_preds, train_labels = [], []\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"].to(DEVICE),\n",
    "                batch[\"attention_mask\"].to(DEVICE),\n",
    "                batch[\"label\"].to(DEVICE),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute train F1 scores\n",
    "        train_macro_f1 = f1_score(train_labels, train_preds, average=\"macro\")\n",
    "        train_weighted_f1 = f1_score(train_labels, train_preds, average=\"weighted\")\n",
    "\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Train Macro-F1: {train_macro_f1:.4f}, Weighted-F1: {train_weighted_f1:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        val_loss, val_macro_f1, val_weighted_f1 = evaluate_model(model, val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro-F1: {val_macro_f1:.4f}, Weighted-F1: {val_weighted_f1:.4f}\")\n",
    "\n",
    "        f1_hm = 2 * val_macro_f1 * val_weighted_f1 / (val_macro_f1 + val_weighted_f1)\n",
    "\n",
    "        # Save best model\n",
    "        if f1_hm > best_f1:\n",
    "            best_f1 = f1_hm\n",
    "            torch.save(model.state_dict(), f\"{model_save_name}_anxiety_model.pth\")\n",
    "            print(\"Best model saved!\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids, attention_mask, labels_batch = (\n",
    "                batch[\"input_ids\"].to(DEVICE),\n",
    "                batch[\"attention_mask\"].to(DEVICE),\n",
    "                batch[\"label\"].to(DEVICE),\n",
    "            )\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, labels=labels_batch)\n",
    "            loss += outputs.loss.item()\n",
    "\n",
    "            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    return loss / len(loader), macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + BERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:29:44.773344Z",
     "iopub.status.busy": "2025-03-25T13:29:44.773121Z",
     "iopub.status.idle": "2025-03-25T13:38:47.534132Z",
     "shell.execute_reply": "2025-03-25T13:38:47.533289Z",
     "shell.execute_reply.started": "2025-03-25T13:29:44.773325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 2153\n",
      "Validation Set Size: 307\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:51<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6225\n",
      "Train Macro-F1: 0.3323, Weighted-F1: 0.3356\n",
      "Validation Loss: 1.3673\n",
      "Validation Macro-F1: 0.4738, Weighted-F1: 0.4769\n",
      "Best model saved!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1457\n",
      "Train Macro-F1: 0.6061, Weighted-F1: 0.6074\n",
      "Validation Loss: 1.0931\n",
      "Validation Macro-F1: 0.5934, Weighted-F1: 0.5931\n",
      "Best model saved!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:51<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7792\n",
      "Train Macro-F1: 0.7548, Weighted-F1: 0.7556\n",
      "Validation Loss: 1.0520\n",
      "Validation Macro-F1: 0.6285, Weighted-F1: 0.6257\n",
      "Best model saved!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4366\n",
      "Train Macro-F1: 0.8778, Weighted-F1: 0.8788\n",
      "Validation Loss: 1.1325\n",
      "Validation Macro-F1: 0.6252, Weighted-F1: 0.6241\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2542\n",
      "Train Macro-F1: 0.9356, Weighted-F1: 0.9365\n",
      "Validation Loss: 1.2089\n",
      "Validation Macro-F1: 0.6486, Weighted-F1: 0.6436\n",
      "Best model saved!\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1527\n",
      "Train Macro-F1: 0.9649, Weighted-F1: 0.9652\n",
      "Validation Loss: 1.3599\n",
      "Validation Macro-F1: 0.5943, Weighted-F1: 0.5928\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1119\n",
      "Train Macro-F1: 0.9751, Weighted-F1: 0.9754\n",
      "Validation Loss: 1.4118\n",
      "Validation Macro-F1: 0.6313, Weighted-F1: 0.6289\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0881\n",
      "Train Macro-F1: 0.9806, Weighted-F1: 0.9810\n",
      "Validation Loss: 1.4800\n",
      "Validation Macro-F1: 0.6351, Weighted-F1: 0.6330\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0697\n",
      "Train Macro-F1: 0.9808, Weighted-F1: 0.9810\n",
      "Validation Loss: 1.7132\n",
      "Validation Macro-F1: 0.6031, Weighted-F1: 0.5997\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0590\n",
      "Train Macro-F1: 0.9836, Weighted-F1: 0.9837\n",
      "Validation Loss: 1.5479\n",
      "Validation Macro-F1: 0.6254, Weighted-F1: 0.6238\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES).to(DEVICE)\n",
    "model_bert, tokenizer_bert = train_model(model, model_name, train_data, val_data, EPOCHS, \"bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:39:34.688926Z",
     "iopub.status.busy": "2025-03-25T13:39:34.688632Z",
     "iopub.status.idle": "2025-03-25T13:39:40.338802Z",
     "shell.execute_reply": "2025-03-25T13:39:40.337825Z",
     "shell.execute_reply.started": "2025-03-25T13:39:34.688900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Size: 615\n",
      "\n",
      "Final Test Evaluation:\n",
      "Test Loss: 1.2925\n",
      "Test Macro-F1: 0.6163\n",
      "Test Weighted-F1: 0.6143\n"
     ]
    }
   ],
   "source": [
    "test_dataset_bert = AnxietyDataset(test_data, tokenizer_bert, LABEL_MAP)\n",
    "print(\"Test Set Size:\", len(test_dataset_bert))\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_bert.load_state_dict(torch.load(\"bert_anxiety_model.pth\", weights_only=True))\n",
    "test_loss, test_macro_f1, test_weighted_f1 = evaluate_model(model_bert, test_loader_bert)\n",
    "\n",
    "print(f\"\\nFinal Test Evaluation:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
    "print(f\"Test Weighted-F1: {test_weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + Mental-BERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:39:45.002485Z",
     "iopub.status.busy": "2025-03-25T13:39:45.002227Z",
     "iopub.status.idle": "2025-03-25T13:48:42.168937Z",
     "shell.execute_reply": "2025-03-25T13:48:42.168058Z",
     "shell.execute_reply.started": "2025-03-25T13:39:45.002465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 2153\n",
      "Validation Set Size: 307\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:52<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5044\n",
      "Train Macro-F1: 0.4281, Weighted-F1: 0.4288\n",
      "Validation Loss: 1.1532\n",
      "Validation Macro-F1: 0.5849, Weighted-F1: 0.5821\n",
      "Best model saved!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9034\n",
      "Train Macro-F1: 0.6969, Weighted-F1: 0.6958\n",
      "Validation Loss: 0.9703\n",
      "Validation Macro-F1: 0.6476, Weighted-F1: 0.6450\n",
      "Best model saved!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5224\n",
      "Train Macro-F1: 0.8423, Weighted-F1: 0.8418\n",
      "Validation Loss: 1.0567\n",
      "Validation Macro-F1: 0.6361, Weighted-F1: 0.6305\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2471\n",
      "Train Macro-F1: 0.9356, Weighted-F1: 0.9355\n",
      "Validation Loss: 1.1351\n",
      "Validation Macro-F1: 0.6473, Weighted-F1: 0.6437\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1420\n",
      "Train Macro-F1: 0.9645, Weighted-F1: 0.9647\n",
      "Validation Loss: 1.3002\n",
      "Validation Macro-F1: 0.6335, Weighted-F1: 0.6311\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0830\n",
      "Train Macro-F1: 0.9817, Weighted-F1: 0.9819\n",
      "Validation Loss: 1.4449\n",
      "Validation Macro-F1: 0.6310, Weighted-F1: 0.6270\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0769\n",
      "Train Macro-F1: 0.9823, Weighted-F1: 0.9823\n",
      "Validation Loss: 1.3914\n",
      "Validation Macro-F1: 0.6315, Weighted-F1: 0.6280\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0535\n",
      "Train Macro-F1: 0.9841, Weighted-F1: 0.9842\n",
      "Validation Loss: 1.4877\n",
      "Validation Macro-F1: 0.6149, Weighted-F1: 0.6110\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0503\n",
      "Train Macro-F1: 0.9866, Weighted-F1: 0.9865\n",
      "Validation Loss: 1.5840\n",
      "Validation Macro-F1: 0.6055, Weighted-F1: 0.6023\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:50<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0376\n",
      "Train Macro-F1: 0.9874, Weighted-F1: 0.9875\n",
      "Validation Loss: 1.6814\n",
      "Validation Macro-F1: 0.6217, Weighted-F1: 0.6176\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mental/mental-bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES).to(DEVICE)\n",
    "model_mental_bert, tokenizer_mental_bert = train_model(model, model_name, train_data, val_data, EPOCHS, \"mental_bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + Mental-BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T13:48:42.171144Z",
     "iopub.status.busy": "2025-03-25T13:48:42.170828Z",
     "iopub.status.idle": "2025-03-25T13:48:47.818908Z",
     "shell.execute_reply": "2025-03-25T13:48:47.818096Z",
     "shell.execute_reply.started": "2025-03-25T13:48:42.171105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Size: 615\n",
      "\n",
      "Final Test Evaluation:\n",
      "Test Loss: 1.0819\n",
      "Test Macro-F1: 0.6235\n",
      "Test Weighted-F1: 0.6232\n"
     ]
    }
   ],
   "source": [
    "test_dataset_mental_bert = AnxietyDataset(test_data, tokenizer_mental_bert, LABEL_MAP)\n",
    "print(\"Test Set Size:\", len(test_dataset_mental_bert))\n",
    "test_loader_mental_bert = DataLoader(test_dataset_mental_bert, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_mental_bert.load_state_dict(torch.load(\"mental_bert_anxiety_model.pth\", weights_only=True))\n",
    "test_loss, test_macro_f1, test_weighted_f1 = evaluate_model(model_mental_bert, test_loader_mental_bert)\n",
    "\n",
    "print(f\"\\nFinal Test Evaluation:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
    "print(f\"Test Weighted-F1: {test_weighted_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6954992,
     "sourceId": 11148254,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6964274,
     "sourceId": 11161008,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
