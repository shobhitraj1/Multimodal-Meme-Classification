{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Checkpoint Drive Link: ([BERT](https://drive.google.com/file/d/1AAQlwZGk0XEMPaMdJWMctXTqUVCKr5pm/view?usp=drive_link) and [Mental-BERT](https://drive.google.com/file/d/1R-WApOMkNu-Hm1IAQ8frI83dJT_A3jgh/view?usp=drive_link))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T14:16:55.960126Z",
     "iopub.status.busy": "2025-03-25T14:16:55.959779Z",
     "iopub.status.idle": "2025-03-25T14:16:55.964556Z",
     "shell.execute_reply": "2025-03-25T14:16:55.963722Z",
     "shell.execute_reply.started": "2025-03-25T14:16:55.960100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T14:36:26.583787Z",
     "iopub.status.busy": "2025-03-25T14:36:26.583467Z",
     "iopub.status.idle": "2025-03-25T14:36:26.588209Z",
     "shell.execute_reply": "2025-03-25T14:36:26.587412Z",
     "shell.execute_reply.started": "2025-03-25T14:36:26.583760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "LABELS = [\"Lack of Interest\", \"Feeling Down\", \"Eating Disorder\",\n",
    "          \"Sleeping Disorder\", \"Low Self-Esteem\", \"Concentration Problem\", \"Self-Harm\"]\n",
    "LABEL_MAP = {label: i for i, label in enumerate(LABELS)}\n",
    "NUM_CLASSES = len(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T14:07:44.213872Z",
     "iopub.status.busy": "2025-03-25T14:07:44.213544Z",
     "iopub.status.idle": "2025-03-25T14:07:44.219531Z",
     "shell.execute_reply": "2025-03-25T14:07:44.218629Z",
     "shell.execute_reply.started": "2025-03-25T14:07:44.213847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DepressionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_map, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        text = sample[\"ocr_text\"]\n",
    "        labels = sample[\"meme_depressive_categories\"]\n",
    "\n",
    "        # Convert label list to one-hot encoding\n",
    "        label_tensor = torch.zeros(len(self.label_map))\n",
    "        for label in labels:\n",
    "            label_tensor[self.label_map[label]] = 1\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": label_tensor,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T14:11:47.317911Z",
     "iopub.status.busy": "2025-03-25T14:11:47.317609Z",
     "iopub.status.idle": "2025-03-25T14:11:47.450592Z",
     "shell.execute_reply": "2025-03-25T14:11:47.449888Z",
     "shell.execute_reply.started": "2025-03-25T14:11:47.317889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = json.load(open(\"/kaggle/input/depression-dataset/depression_train.json\", \"r\"))\n",
    "val_data = json.load(open(\"/kaggle/input/depression-dataset/depression_val.json\", \"r\"))\n",
    "test_data = json.load(open(\"/kaggle/input/depression-dataset/depression_test.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T14:19:31.656096Z",
     "iopub.status.busy": "2025-03-25T14:19:31.655748Z",
     "iopub.status.idle": "2025-03-25T14:19:31.661410Z",
     "shell.execute_reply": "2025-03-25T14:19:31.660606Z",
     "shell.execute_reply.started": "2025-03-25T14:19:31.656068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Data: 8722\n",
      "Size of Validation Data: 359\n",
      "Size of Test Data: 520\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training Data:\", len(train_data))\n",
    "print(\"Size of Validation Data:\", len(val_data))\n",
    "print(\"Size of Test Data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T14:17:07.477156Z",
     "iopub.status.busy": "2025-03-25T14:17:07.476876Z",
     "iopub.status.idle": "2025-03-25T14:17:07.488795Z",
     "shell.execute_reply": "2025-03-25T14:17:07.487901Z",
     "shell.execute_reply.started": "2025-03-25T14:17:07.477133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Train dataset:\n",
      "  Lack of Interest: 471\n",
      "  Feeling Down: 2085\n",
      "  Eating Disorder: 1939\n",
      "  Sleeping Disorder: 1562\n",
      "  Low Self-Esteem: 855\n",
      "  Concentration Problem: 595\n",
      "  Self-Harm: 1516\n",
      "-----------------------------------\n",
      "Class distribution in Validation dataset:\n",
      "  Lack of Interest: 45\n",
      "  Feeling Down: 195\n",
      "  Eating Disorder: 49\n",
      "  Sleeping Disorder: 45\n",
      "  Low Self-Esteem: 85\n",
      "  Concentration Problem: 42\n",
      "  Self-Harm: 61\n",
      "-----------------------------------\n",
      "Class distribution in Test dataset:\n",
      "  Lack of Interest: 71\n",
      "  Feeling Down: 218\n",
      "  Eating Disorder: 92\n",
      "  Sleeping Disorder: 79\n",
      "  Low Self-Esteem: 114\n",
      "  Concentration Problem: 66\n",
      "  Self-Harm: 81\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_class_distribution(dataset, dataset_name):\n",
    "    label_counts = Counter([LABEL_MAP[i] for item in dataset for i in item[\"meme_depressive_categories\"]])\n",
    "    print(f\"Class distribution in {dataset_name} dataset:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"  {LABELS[label]}: {count}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "print_class_distribution(train_data, \"Train\")\n",
    "print_class_distribution(val_data, \"Validation\")\n",
    "print_class_distribution(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T14:40:09.809462Z",
     "iopub.status.busy": "2025-03-25T14:40:09.809096Z",
     "iopub.status.idle": "2025-03-25T14:40:09.820471Z",
     "shell.execute_reply": "2025-03-25T14:40:09.819593Z",
     "shell.execute_reply.started": "2025-03-25T14:40:09.809431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_data, val_data, epochs, model_save_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = DepressionDataset(train_data, tokenizer, LABEL_MAP)\n",
    "    val_dataset = DepressionDataset(val_data, tokenizer, LABEL_MAP)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"].to(DEVICE),\n",
    "                batch[\"attention_mask\"].to(DEVICE),\n",
    "                batch[\"labels\"].to(DEVICE),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, labels=labels)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.sigmoid(outputs.logits).detach().cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute train F1 scores\n",
    "        train_macro_f1, train_weighted_f1 = compute_f1_scores(train_labels, train_preds)\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Train Macro-F1: {train_macro_f1:.4f}, Weighted-F1: {train_weighted_f1:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        val_loss, val_macro_f1, val_weighted_f1 = evaluate_model(model, val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro-F1: {val_macro_f1:.4f}, Weighted-F1: {val_weighted_f1:.4f}\")\n",
    "\n",
    "        f1_hm = 2 * val_macro_f1 * val_weighted_f1 / (val_macro_f1 + val_weighted_f1)\n",
    "\n",
    "        # Save best model\n",
    "        if f1_hm > best_val_f1:\n",
    "            best_val_f1 = f1_hm\n",
    "            torch.save(model.state_dict(), f\"{model_save_name}_depression_model.pth\")\n",
    "            print(\"Best model saved!\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    preds, labels = [], []\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids, attention_mask, labels_batch = (\n",
    "                batch[\"input_ids\"].to(DEVICE),\n",
    "                batch[\"attention_mask\"].to(DEVICE),\n",
    "                batch[\"labels\"].to(DEVICE),\n",
    "            )\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, labels=labels_batch)\n",
    "            loss += criterion(outputs.logits, labels_batch).item()\n",
    "\n",
    "            preds.extend(torch.sigmoid(outputs.logits).cpu().numpy())\n",
    "            labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "    macro_f1, weighted_f1 = compute_f1_scores(labels, preds)\n",
    "    return loss / len(loader), macro_f1, weighted_f1\n",
    "\n",
    "def compute_f1_scores(true_labels, pred_probs, threshold=0.5):\n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_probs = np.array(pred_probs)\n",
    "    pred_labels = (pred_probs > threshold).astype(int)\n",
    "\n",
    "    macro_f1 = f1_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    weighted_f1 = f1_score(true_labels, pred_labels, average=\"weighted\", zero_division=0)\n",
    "    return macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + BERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T14:40:12.158339Z",
     "iopub.status.busy": "2025-03-25T14:40:12.158052Z",
     "iopub.status.idle": "2025-03-25T15:15:39.446044Z",
     "shell.execute_reply": "2025-03-25T15:15:39.444986Z",
     "shell.execute_reply.started": "2025-03-25T14:40:12.158317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3110\n",
      "Train Macro-F1: 0.4025, Weighted-F1: 0.4765\n",
      "Validation Loss: 0.3648\n",
      "Validation Macro-F1: 0.5411, Weighted-F1: 0.5406\n",
      "Best model saved!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1886\n",
      "Train Macro-F1: 0.7333, Weighted-F1: 0.7553\n",
      "Validation Loss: 0.3551\n",
      "Validation Macro-F1: 0.5465, Weighted-F1: 0.5500\n",
      "Best model saved!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1323\n",
      "Train Macro-F1: 0.8193, Weighted-F1: 0.8390\n",
      "Validation Loss: 0.3932\n",
      "Validation Macro-F1: 0.6068, Weighted-F1: 0.6143\n",
      "Best model saved!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0894\n",
      "Train Macro-F1: 0.8868, Weighted-F1: 0.8998\n",
      "Validation Loss: 0.4475\n",
      "Validation Macro-F1: 0.5812, Weighted-F1: 0.5816\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0617\n",
      "Train Macro-F1: 0.9295, Weighted-F1: 0.9368\n",
      "Validation Loss: 0.5221\n",
      "Validation Macro-F1: 0.5756, Weighted-F1: 0.5584\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0461\n",
      "Train Macro-F1: 0.9488, Weighted-F1: 0.9530\n",
      "Validation Loss: 0.5211\n",
      "Validation Macro-F1: 0.5966, Weighted-F1: 0.6079\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:28<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0387\n",
      "Train Macro-F1: 0.9589, Weighted-F1: 0.9616\n",
      "Validation Loss: 0.5836\n",
      "Validation Macro-F1: 0.5670, Weighted-F1: 0.5759\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0318\n",
      "Train Macro-F1: 0.9655, Weighted-F1: 0.9680\n",
      "Validation Loss: 0.5709\n",
      "Validation Macro-F1: 0.5951, Weighted-F1: 0.5892\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0276\n",
      "Train Macro-F1: 0.9677, Weighted-F1: 0.9709\n",
      "Validation Loss: 0.6540\n",
      "Validation Macro-F1: 0.5677, Weighted-F1: 0.5632\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0239\n",
      "Train Macro-F1: 0.9725, Weighted-F1: 0.9744\n",
      "Validation Loss: 0.6326\n",
      "Validation Macro-F1: 0.5827, Weighted-F1: 0.5903\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES, problem_type=\"multi_label_classification\").to(DEVICE)\n",
    "model_bert, tokenizer_bert = train_model(model, model_name, train_data, val_data, EPOCHS, \"bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:16:02.912334Z",
     "iopub.status.busy": "2025-03-25T15:16:02.912021Z",
     "iopub.status.idle": "2025-03-25T15:16:07.515482Z",
     "shell.execute_reply": "2025-03-25T15:16:07.514349Z",
     "shell.execute_reply.started": "2025-03-25T15:16:02.912310Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Evaluation:\n",
      "Test Loss: 0.3503\n",
      "Test Macro-F1: 0.6355\n",
      "Test Weighted-F1: 0.6347\n"
     ]
    }
   ],
   "source": [
    "test_dataset_bert = DepressionDataset(test_data, tokenizer_bert, LABEL_MAP)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_bert.load_state_dict(torch.load(\"bert_depression_model.pth\", weights_only=True))\n",
    "test_loss, test_macro_f1, test_weighted_f1 = evaluate_model(model_bert, test_loader_bert)\n",
    "\n",
    "print(f\"Final Test Evaluation:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
    "print(f\"Test Weighted-F1: {test_weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + Mental-BERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:16:11.194023Z",
     "iopub.status.busy": "2025-03-25T15:16:11.193730Z",
     "iopub.status.idle": "2025-03-25T15:51:41.333759Z",
     "shell.execute_reply": "2025-03-25T15:51:41.332772Z",
     "shell.execute_reply.started": "2025-03-25T15:16:11.194000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caeccb95474440cba89f08d0963efd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/639 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d970939c7d40f8bb9c36d1e152c24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298badf028914d1cb09fc5394218fd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2539a3d5e24522824cded4d78f3b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27862ac07fb64b2fb7db8a24794e2b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f162bc8f1bab48d394ee22ce3b8ca061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:30<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2761\n",
      "Train Macro-F1: 0.5307, Weighted-F1: 0.5681\n",
      "Validation Loss: 0.3870\n",
      "Validation Macro-F1: 0.5568, Weighted-F1: 0.5435\n",
      "Best model saved!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1745\n",
      "Train Macro-F1: 0.7519, Weighted-F1: 0.7705\n",
      "Validation Loss: 0.3960\n",
      "Validation Macro-F1: 0.5722, Weighted-F1: 0.5565\n",
      "Best model saved!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1228\n",
      "Train Macro-F1: 0.8406, Weighted-F1: 0.8541\n",
      "Validation Loss: 0.4455\n",
      "Validation Macro-F1: 0.5944, Weighted-F1: 0.5729\n",
      "Best model saved!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:28<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0813\n",
      "Train Macro-F1: 0.9032, Weighted-F1: 0.9124\n",
      "Validation Loss: 0.5153\n",
      "Validation Macro-F1: 0.5796, Weighted-F1: 0.5719\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:28<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0543\n",
      "Train Macro-F1: 0.9384, Weighted-F1: 0.9442\n",
      "Validation Loss: 0.4937\n",
      "Validation Macro-F1: 0.5966, Weighted-F1: 0.6014\n",
      "Best model saved!\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:28<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0428\n",
      "Train Macro-F1: 0.9527, Weighted-F1: 0.9565\n",
      "Validation Loss: 0.5180\n",
      "Validation Macro-F1: 0.5977, Weighted-F1: 0.6065\n",
      "Best model saved!\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0352\n",
      "Train Macro-F1: 0.9618, Weighted-F1: 0.9636\n",
      "Validation Loss: 0.5967\n",
      "Validation Macro-F1: 0.5766, Weighted-F1: 0.5587\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:28<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0267\n",
      "Train Macro-F1: 0.9711, Weighted-F1: 0.9730\n",
      "Validation Loss: 0.5951\n",
      "Validation Macro-F1: 0.5902, Weighted-F1: 0.5906\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:28<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0232\n",
      "Train Macro-F1: 0.9756, Weighted-F1: 0.9766\n",
      "Validation Loss: 0.5983\n",
      "Validation Macro-F1: 0.6035, Weighted-F1: 0.6112\n",
      "Best model saved!\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:29<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0203\n",
      "Train Macro-F1: 0.9770, Weighted-F1: 0.9783\n",
      "Validation Loss: 0.6248\n",
      "Validation Macro-F1: 0.5866, Weighted-F1: 0.6004\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mental/mental-bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES, problem_type=\"multi_label_classification\").to(DEVICE)\n",
    "model_mental_bert, tokenizer_mental_bert = train_model(model, model_name, train_data, val_data, EPOCHS, \"mental_bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR + Mental-BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T15:51:41.335547Z",
     "iopub.status.busy": "2025-03-25T15:51:41.335270Z",
     "iopub.status.idle": "2025-03-25T15:51:46.038338Z",
     "shell.execute_reply": "2025-03-25T15:51:46.037395Z",
     "shell.execute_reply.started": "2025-03-25T15:51:41.335523Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Evaluation:\n",
      "Test Loss: 0.5416\n",
      "Test Macro-F1: 0.6313\n",
      "Test Weighted-F1: 0.6249\n"
     ]
    }
   ],
   "source": [
    "test_dataset_mental_bert = DepressionDataset(test_data, tokenizer_mental_bert, LABEL_MAP)\n",
    "test_loader_mental_bert = DataLoader(test_dataset_mental_bert, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_mental_bert.load_state_dict(torch.load(\"mental_bert_depression_model.pth\", weights_only=True))\n",
    "test_loss, test_macro_f1, test_weighted_f1 = evaluate_model(model_mental_bert, test_loader_mental_bert)\n",
    "\n",
    "print(f\"\\nFinal Test Evaluation:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
    "print(f\"Test Weighted-F1: {test_weighted_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6965270,
     "sourceId": 11162322,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
